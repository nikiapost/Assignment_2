{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9827f35f",
   "metadata": {},
   "source": [
    "#### Read .csv to see the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57644ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            HITId                       HITTypeId  \\\n",
      "0  3HEA4ZVWVDGYCTW1R8LCMPUM6A4555  3IFS5X633EQ4LQRZVD7FKABVXAK9KN   \n",
      "1  3HEA4ZVWVDGYCTW1R8LCMPUM6A4555  3IFS5X633EQ4LQRZVD7FKABVXAK9KN   \n",
      "2  3HEA4ZVWVDGYCTW1R8LCMPUM6A4555  3IFS5X633EQ4LQRZVD7FKABVXAK9KN   \n",
      "3  3HEA4ZVWVDGYCTW1R8LCMPUM6A4555  3IFS5X633EQ4LQRZVD7FKABVXAK9KN   \n",
      "4  3HEA4ZVWVDGYCTW1R8LCMPUM6A4555  3IFS5X633EQ4LQRZVD7FKABVXAK9KN   \n",
      "\n",
      "                                               Title  \\\n",
      "0  Create a dialog on action movies for ten sente...   \n",
      "1  Create a dialog on action movies for ten sente...   \n",
      "2  Create a dialog on action movies for ten sente...   \n",
      "3  Create a dialog on action movies for ten sente...   \n",
      "4  Create a dialog on action movies for ten sente...   \n",
      "\n",
      "                                         Description  \\\n",
      "0  Create a dialog on action movies for ten sente...   \n",
      "1  Create a dialog on action movies for ten sente...   \n",
      "2  Create a dialog on action movies for ten sente...   \n",
      "3  Create a dialog on action movies for ten sente...   \n",
      "4  Create a dialog on action movies for ten sente...   \n",
      "\n",
      "                                            Keywords Reward  \\\n",
      "0  dialog, chat, movies, film, entertainment, art...  $0.40   \n",
      "1  dialog, chat, movies, film, entertainment, art...  $0.40   \n",
      "2  dialog, chat, movies, film, entertainment, art...  $0.40   \n",
      "3  dialog, chat, movies, film, entertainment, art...  $0.40   \n",
      "4  dialog, chat, movies, film, entertainment, art...  $0.40   \n",
      "\n",
      "                   CreationTime  MaxAssignments  \\\n",
      "0  Mon Jun 19 10:43:22 PDT 2017               9   \n",
      "1  Mon Jun 19 10:43:22 PDT 2017               9   \n",
      "2  Mon Jun 19 10:43:22 PDT 2017               9   \n",
      "3  Mon Jun 19 10:43:22 PDT 2017               9   \n",
      "4  Mon Jun 19 10:43:22 PDT 2017               9   \n",
      "\n",
      "                                RequesterAnnotation  \\\n",
      "0  BatchId:2840983;OriginalHitTemplateId:920937340;   \n",
      "1  BatchId:2840983;OriginalHitTemplateId:920937340;   \n",
      "2  BatchId:2840983;OriginalHitTemplateId:920937340;   \n",
      "3  BatchId:2840983;OriginalHitTemplateId:920937340;   \n",
      "4  BatchId:2840983;OriginalHitTemplateId:920937340;   \n",
      "\n",
      "   AssignmentDurationInSeconds  ...  \\\n",
      "0                         7200  ...   \n",
      "1                         7200  ...   \n",
      "2                         7200  ...   \n",
      "3                         7200  ...   \n",
      "4                         7200  ...   \n",
      "\n",
      "                                    Answer.sentence2  \\\n",
      "0  i have and i am so in love with the trailer al...   \n",
      "1                                      Yeah, me too.   \n",
      "2                                I actually liked it   \n",
      "3                                      What is that?   \n",
      "4                                  Yes it was great.   \n",
      "\n",
      "                                    Answer.sentence3  \\\n",
      "0                        it looks remarkable so far!   \n",
      "1  Matt Damon really turned the spy genre upside ...   \n",
      "2                                           You did?   \n",
      "3  It's Tom Cruise and Emily Blunt dying every da...   \n",
      "4  I loved the beginning when the nazi was riding...   \n",
      "\n",
      "                                    Answer.sentence4  \\\n",
      "0  chadwick really is a good actor for black pant...   \n",
      "1                                            How so?   \n",
      "2         Yeah i heard so many people do not like it   \n",
      "3                           That's what it's called?   \n",
      "4  Oh yes that was great, but how about the panze...   \n",
      "\n",
      "                                    Answer.sentence5  \\\n",
      "0       i agree!. he really is suitable for the role   \n",
      "1  He created a spy that had human flaws and wasn...   \n",
      "2                                         Why though   \n",
      "3                                              Yeah.   \n",
      "4  Yes the Panzer fight was cool, but that german...   \n",
      "\n",
      "                      Answer.sentence6  \\\n",
      "0  the trailer was kind of sad though.   \n",
      "1          I can see how that is true.   \n",
      "2           I think they hate the plot   \n",
      "3          I saw it but not that name.   \n",
      "4   Yes that was graphic for my taste.   \n",
      "\n",
      "                                    Answer.sentence7  \\\n",
      "0    how so? did it bother you when his father died?   \n",
      "1        The first one in the series is my favorite.   \n",
      "2                             i enjoyed it sorta too   \n",
      "3                                       What was it?   \n",
      "4  Then those poor nazi in the building they shot...   \n",
      "\n",
      "                                    Answer.sentence8  \\\n",
      "0  yes, it did, but i hope he become one of the b...   \n",
      "1                                          Mine too.   \n",
      "2  Yeah it was nowhere near as bad as it was made...   \n",
      "3                                 Live. Die. Repeat.   \n",
      "4  Yes I think the gunner took pity on them when ...   \n",
      "\n",
      "                                    Answer.sentence9 Approve Reject  \n",
      "0                           i second that statement!     NaN    NaN  \n",
      "1  We really get to know Matt Damon's character f...     NaN    NaN  \n",
      "2                                            Exactly     NaN    NaN  \n",
      "3  Oooh, they renamed it because the studio thoug...     NaN    NaN  \n",
      "4                      I hated to see them all, die.     NaN    NaN  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'C:/Users/nikia/Desktop/ΔΠΜΣ-ΓΛΩΣΣΙΚΗ ΤΕΧΝΟΛΟΓΙΑ/3ο εξάμηνο/Διαλογικά Συστήματα +Φωνητικοί Βοηθοί/εργασιες/2η/self_dialogue_corpus-main/self_dialogue_corpus-main/corpus/action/Batch_2840983_batch_results.csv'\n",
    "\n",
    "# .csv file into DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02fc4b",
   "metadata": {},
   "source": [
    "#### Total data length in terms of dialogues, turns, sentences, words (no seperation user/system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbdfb360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data Length:\n",
      "Total Dialogues: 24249\n",
      "Total Turns: 1455\n",
      "Total Sentences: 958803\n",
      "Total Words: 5694981\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "corpus_directory = 'C:/Users/nikia/Desktop/ΔΠΜΣ-ΓΛΩΣΣΙΚΗ ΤΕΧΝΟΛΟΓΙΑ/3ο εξάμηνο/Διαλογικά Συστήματα +Φωνητικοί Βοηθοί/εργασιες/2η/self_dialogue_corpus-main/self_dialogue_corpus-main/corpus/'\n",
    "\n",
    "# initialization\n",
    "total_dialogues = 0\n",
    "total_turns = 0\n",
    "total_sentences = 0\n",
    "total_words = 0\n",
    "\n",
    "# iterate through each subfile (topic) in the corpus directory\n",
    "for topic in os.listdir(corpus_directory):\n",
    "    topic_path = os.path.join(corpus_directory, topic)\n",
    "    \n",
    "    if os.path.isdir(topic_path):  # if it's a directory (subfile)\n",
    "        # iterate through each CSV file in the subfile (topic)\n",
    "        for file in os.listdir(topic_path):\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(topic_path, file)\n",
    "                data = pd.read_csv(file_path)\n",
    "                \n",
    "                # Summing up the total dialogues, turns, sentences, and words for each file\n",
    "                total_dialogues += len(data)\n",
    "                total_turns += len(data.columns) // 2  \n",
    "                total_sentences += data.count().sum()\n",
    "                total_words += data.astype(str).apply(lambda x: x.str.split().str.len()).sum().sum()\n",
    "\n",
    "print(\"Total Data Length:\")\n",
    "print(f\"Total Dialogues: {total_dialogues}\")\n",
    "print(f\"Total Turns: {total_turns}\")\n",
    "print(f\"Total Sentences: {total_sentences}\")\n",
    "print(f\"Total Words: {total_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a7a0d0",
   "metadata": {},
   "source": [
    "#### Mean/std dev dialogue lengths in terms of dialogues, turns, sentences, words (no seperation user/system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b29ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean/Std Dev Dialogue Lengths:\n",
      "Mean Dialogue Length (Dialogues): 384.9047619047619\n",
      "StdDev Dialogue Length (Dialogues): 258.1223233020501\n",
      "Mean Dialogue Length (Turns): 22.740154233164255\n",
      "StdDev Dialogue Length (Turns): 2.546752108033262\n",
      "Mean Dialogue Length (Sentences): 39.53989855251763\n",
      "StdDev Dialogue Length (Sentences): 5.128629772166213\n",
      "Mean Dialogue Length (Words): 234.85426203142399\n",
      "StdDev Dialogue Length (Words): 74.36175345292521\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "corpus_directory = 'C:/Users/nikia/Desktop/ΔΠΜΣ-ΓΛΩΣΣΙΚΗ ΤΕΧΝΟΛΟΓΙΑ/3ο εξάμηνο/Διαλογικά Συστήματα +Φωνητικοί Βοηθοί/εργασιες/2η/self_dialogue_corpus-main/self_dialogue_corpus-main/corpus/'\n",
    "\n",
    "# initialization\n",
    "dialogue_lengths = []\n",
    "turn_lengths = []\n",
    "sentence_lengths = []\n",
    "word_lengths = []\n",
    "\n",
    "# iterate through each subfile (topic) in the corpus directory\n",
    "for topic in os.listdir(corpus_directory):\n",
    "    topic_path = os.path.join(corpus_directory, topic)\n",
    "    \n",
    "    if os.path.isdir(topic_path):  # if it's a directory (subfile)\n",
    "        # iterate through each CSV file in the subfile (topic)\n",
    "        for file in os.listdir(topic_path):\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(topic_path, file)\n",
    "                data = pd.read_csv(file_path)\n",
    "                \n",
    "                # calculating lengths for dialogues, turns, sentences, and words for each file\n",
    "                dialogue_lengths.append(len(data))\n",
    "                turn_lengths.extend([len(data.columns) // 2] * len(data)) \n",
    "                sentence_lengths.extend(data.count(axis=1))\n",
    "                word_lengths.extend(data.astype(str).apply(lambda x: x.str.split().str.len(), axis=1).sum(axis=1))\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean_dialogue_length = np.mean(dialogue_lengths)\n",
    "std_dev_dialogue_length = np.std(dialogue_lengths)\n",
    "\n",
    "mean_turn_length = np.mean(turn_lengths)\n",
    "std_dev_turn_length = np.std(turn_lengths)\n",
    "\n",
    "mean_sentence_length = np.mean(sentence_lengths)\n",
    "std_dev_sentence_length = np.std(sentence_lengths)\n",
    "\n",
    "mean_word_length = np.mean(word_lengths)\n",
    "std_dev_word_length = np.std(word_lengths)\n",
    "\n",
    "print(\"Mean/Std Dev Dialogue Lengths:\")\n",
    "print(f\"Mean Dialogue Length (Dialogues): {mean_dialogue_length}\")\n",
    "print(f\"StdDev Dialogue Length (Dialogues): {std_dev_dialogue_length}\")\n",
    "print(f\"Mean Dialogue Length (Turns): {mean_turn_length}\")\n",
    "print(f\"StdDev Dialogue Length (Turns): {std_dev_turn_length}\")\n",
    "print(f\"Mean Dialogue Length (Sentences): {mean_sentence_length}\")\n",
    "print(f\"StdDev Dialogue Length (Sentences): {std_dev_sentence_length}\")\n",
    "print(f\"Mean Dialogue Length (Words): {mean_word_length}\")\n",
    "print(f\"StdDev Dialogue Length (Words): {std_dev_word_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00517630",
   "metadata": {},
   "source": [
    "#### Mean/std dev dialogue lengths in terms of dialogues, turns, sentences, words (no seperation user/system). PER TOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3544cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean/Std Dev per Topic:\n",
      "dialogue_lengths:\n",
      "  Topic 1 - Mean: 414.0, StdDev: 0.0\n",
      "  Topic 2 - Mean: 124.75, StdDev: 79.04864008950439\n",
      "  Topic 3 - Mean: 97.8, StdDev: 84.1436866318561\n",
      "  Topic 4 - Mean: 684.0, StdDev: 0.0\n",
      "  Topic 5 - Mean: 414.0, StdDev: 0.0\n",
      "  Topic 6 - Mean: 468.0, StdDev: 108.0\n",
      "  Topic 7 - Mean: 70.33333333333333, StdDev: 39.41516910474387\n",
      "  Topic 8 - Mean: 343.0, StdDev: 0.0\n",
      "  Topic 9 - Mean: 414.0, StdDev: 0.0\n",
      "  Topic 10 - Mean: 414.0, StdDev: 0.0\n",
      "  Topic 11 - Mean: 87.5, StdDev: 38.5\n",
      "  Topic 12 - Mean: 558.0, StdDev: 0.0\n",
      "  Topic 13 - Mean: 318.15384615384613, StdDev: 204.49067917590398\n",
      "  Topic 14 - Mean: 546.0, StdDev: 245.63387388550464\n",
      "  Topic 15 - Mean: 216.0, StdDev: 0.0\n",
      "  Topic 16 - Mean: 936.0, StdDev: 63.63961030678928\n",
      "  Topic 17 - Mean: 684.0, StdDev: 0.0\n",
      "  Topic 18 - Mean: 684.0, StdDev: 0.0\n",
      "  Topic 19 - Mean: 684.0, StdDev: 0.0\n",
      "  Topic 20 - Mean: 432.0, StdDev: 166.43917808016238\n",
      "  Topic 21 - Mean: 414.0, StdDev: 0.0\n",
      "  Topic 22 - Mean: 414.0, StdDev: 0.0\n",
      "  Topic 23 - Mean: 198.0, StdDev: 0.0\n",
      "turn_lengths:\n",
      "  Topic 1 - Mean: 20.0, StdDev: 0.0\n",
      "  Topic 2 - Mean: 25.0, StdDev: 0.0\n",
      "  Topic 3 - Mean: 24.995910020449898, StdDev: 0.0638220308152443\n",
      "  Topic 4 - Mean: 20.0, StdDev: 0.0\n",
      "  Topic 5 - Mean: 20.0, StdDev: 0.0\n",
      "  Topic 6 - Mean: 20.0, StdDev: 0.0\n",
      "  Topic 7 - Mean: 25.0, StdDev: 0.0\n",
      "  Topic 8 - Mean: 20.0, StdDev: 0.0\n",
      "  Topic 9 - Mean: 20.0, StdDev: 0.0\n",
      "  Topic 10 - Mean: 20.0, StdDev: 0.0\n",
      "  Topic 11 - Mean: 25.0, StdDev: 0.0\n",
      "  Topic 12 - Mean: 20.0, StdDev: 0.0\n",
      "  Topic 13 - Mean: 25.0, StdDev: 0.0\n",
      "  Topic 14 - Mean: 25.0, StdDev: 0.0\n",
      "  Topic 15 - Mean: 25.0, StdDev: 0.0\n",
      "  Topic 16 - Mean: 25.0, StdDev: 0.0\n",
      "  Topic 17 - Mean: 20.0, StdDev: 0.0\n",
      "  Topic 18 - Mean: 20.0, StdDev: 0.0\n",
      "  Topic 19 - Mean: 20.0, StdDev: 0.0\n",
      "  Topic 20 - Mean: 20.0, StdDev: 0.0\n",
      "  Topic 21 - Mean: 20.0, StdDev: 0.0\n",
      "  Topic 22 - Mean: 20.0, StdDev: 0.0\n",
      "  Topic 23 - Mean: 16.0, StdDev: 0.0\n",
      "sentence_lengths:\n",
      "  Topic 1 - Mean: 33.0, StdDev: 0.0\n",
      "  Topic 2 - Mean: 44.01002004008016, StdDev: 0.09959738388608558\n",
      "  Topic 3 - Mean: 44.02044989775051, StdDev: 0.16796326542642998\n",
      "  Topic 4 - Mean: 34.67690058479532, StdDev: 0.4676603287633806\n",
      "  Topic 5 - Mean: 33.0, StdDev: 0.0\n",
      "  Topic 6 - Mean: 34.05641025641026, StdDev: 0.6380237403327084\n",
      "  Topic 7 - Mean: 44.06635071090047, StdDev: 0.2488941422843769\n",
      "  Topic 8 - Mean: 34.839650145772595, StdDev: 0.3669302092723562\n",
      "  Topic 9 - Mean: 33.0, StdDev: 0.0\n",
      "  Topic 10 - Mean: 33.0, StdDev: 0.0\n",
      "  Topic 11 - Mean: 44.097142857142856, StdDev: 0.29615219473942717\n",
      "  Topic 12 - Mean: 34.68279569892473, StdDev: 0.4653877227158222\n",
      "  Topic 13 - Mean: 43.84308510638298, StdDev: 0.3690001107476237\n",
      "  Topic 14 - Mean: 44.005494505494504, StdDev: 0.40369973174753765\n",
      "  Topic 15 - Mean: 44.0, StdDev: 0.0\n",
      "  Topic 16 - Mean: 44.47649572649573, StdDev: 0.5015817991751579\n",
      "  Topic 17 - Mean: 34.530701754385966, StdDev: 0.49905651210822194\n",
      "  Topic 18 - Mean: 34.62719298245614, StdDev: 0.48355138839001593\n",
      "  Topic 19 - Mean: 34.71637426900585, StdDev: 0.4507573357275342\n",
      "  Topic 20 - Mean: 33.910879629629626, StdDev: 0.6181454973396994\n",
      "  Topic 21 - Mean: 33.0, StdDev: 0.0\n",
      "  Topic 22 - Mean: 34.429951690821255, StdDev: 0.4950689188195913\n",
      "  Topic 23 - Mean: 26.005050505050505, StdDev: 0.07088721640211212\n",
      "word_lengths:\n",
      "  Topic 1 - Mean: 172.45893719806764, StdDev: 38.27849896388863\n",
      "  Topic 2 - Mean: 279.9258517034068, StdDev: 82.80646311546847\n",
      "  Topic 3 - Mean: 278.20245398773005, StdDev: 70.50148959332519\n",
      "  Topic 4 - Mean: 190.71052631578948, StdDev: 39.64972150611484\n",
      "  Topic 5 - Mean: 171.92512077294685, StdDev: 32.5908125054903\n",
      "  Topic 6 - Mean: 188.42606837606837, StdDev: 32.74647635398597\n",
      "  Topic 7 - Mean: 300.09478672985784, StdDev: 74.52968187192818\n",
      "  Topic 8 - Mean: 197.75510204081633, StdDev: 38.389800203745075\n",
      "  Topic 9 - Mean: 198.81159420289856, StdDev: 42.754475550334156\n",
      "  Topic 10 - Mean: 188.46618357487924, StdDev: 44.70096635064253\n",
      "  Topic 11 - Mean: 282.7942857142857, StdDev: 63.39484405739338\n",
      "  Topic 12 - Mean: 177.9910394265233, StdDev: 32.521979201420955\n",
      "  Topic 13 - Mean: 276.92093810444874, StdDev: 72.17398711813061\n",
      "  Topic 14 - Mean: 267.5726495726496, StdDev: 66.41461290029235\n",
      "  Topic 15 - Mean: 260.68518518518516, StdDev: 59.95980878396921\n",
      "  Topic 16 - Mean: 282.59188034188037, StdDev: 76.90096189105282\n",
      "  Topic 17 - Mean: 176.69005847953215, StdDev: 32.314388834195434\n",
      "  Topic 18 - Mean: 188.04093567251462, StdDev: 36.127917106389546\n",
      "  Topic 19 - Mean: 179.2017543859649, StdDev: 34.78379668095496\n",
      "  Topic 20 - Mean: 193.23900462962962, StdDev: 42.53339028133136\n",
      "  Topic 21 - Mean: 185.95410628019323, StdDev: 40.989928302802596\n",
      "  Topic 22 - Mean: 180.30434782608697, StdDev: 39.13665677854615\n",
      "  Topic 23 - Mean: 114.95454545454545, StdDev: 7.4123126167709605\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "corpus_directory = 'C:/Users/nikia/Desktop/ΔΠΜΣ-ΓΛΩΣΣΙΚΗ ΤΕΧΝΟΛΟΓΙΑ/3ο εξάμηνο/Διαλογικά Συστήματα +Φωνητικοί Βοηθοί/εργασιες/2η/self_dialogue_corpus-main/self_dialogue_corpus-main/corpus/'\n",
    "\n",
    "# initialization dictionaries to store statistics per topic\n",
    "topics_statistics = {\n",
    "    'dialogue_lengths': [],\n",
    "    'turn_lengths': [],\n",
    "    'sentence_lengths': [],\n",
    "    'word_lengths': []\n",
    "}\n",
    "\n",
    "# iterate through each subfile (topic) in the corpus directory\n",
    "for topic in os.listdir(corpus_directory):\n",
    "    topic_path = os.path.join(corpus_directory, topic)\n",
    "    \n",
    "    if os.path.isdir(topic_path):  # if it's a directory (subfile)\n",
    "        # initialize lists to store lengths of dialogues, turns, sentences, and words for each topic\n",
    "        dialogue_lengths = []\n",
    "        turn_lengths = []\n",
    "        sentence_lengths = []\n",
    "        word_lengths = []\n",
    "        \n",
    "        # iterate through each CSV file in the subfile (topic)\n",
    "        for file in os.listdir(topic_path):\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(topic_path, file)\n",
    "                data = pd.read_csv(file_path)\n",
    "                \n",
    "                # calculating lengths for dialogues, turns, sentences, and words for each file\n",
    "                dialogue_lengths.append(len(data))\n",
    "                turn_lengths.extend([len(data.columns) // 2] * len(data)) \n",
    "                sentence_lengths.extend(data.count(axis=1))\n",
    "                word_lengths.extend(data.astype(str).apply(lambda x: x.str.split().str.len(), axis=1).sum(axis=1))\n",
    "        \n",
    "        # store statistics for each topic in the dictionary\n",
    "        topics_statistics['dialogue_lengths'].append(dialogue_lengths)\n",
    "        topics_statistics['turn_lengths'].append(turn_lengths)\n",
    "        topics_statistics['sentence_lengths'].append(sentence_lengths)\n",
    "        topics_statistics['word_lengths'].append(word_lengths)\n",
    "\n",
    "# calculate mean and standard deviation per topic\n",
    "topic_mean_std = {}\n",
    "for key, values in topics_statistics.items():\n",
    "    means = [np.mean(val) for val in values]\n",
    "    std_devs = [np.std(val) for val in values]\n",
    "    topic_mean_std[key] = (means, std_devs)\n",
    "\n",
    "print(\"Mean/Std Dev per Topic:\")\n",
    "for key, (means, std_devs) in topic_mean_std.items():\n",
    "    print(f\"{key}:\")\n",
    "    for i, (mean, std_dev) in enumerate(zip(means, std_devs), 1):\n",
    "        print(f\"  Topic {i} - Mean: {mean}, StdDev: {std_dev}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431cb775",
   "metadata": {},
   "source": [
    "#### Total Vocabulary size (no separation user/system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2da751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:\n",
      "Total Vocabulary Size: 117187\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "corpus_directory = 'C:/Users/nikia/Desktop/ΔΠΜΣ-ΓΛΩΣΣΙΚΗ ΤΕΧΝΟΛΟΓΙΑ/3ο εξάμηνο/Διαλογικά Συστήματα +Φωνητικοί Βοηθοί/εργασιες/2η/self_dialogue_corpus-main/self_dialogue_corpus-main/corpus/'\n",
    "\n",
    "# initialization of a set to store unique words\n",
    "unique_words = set()\n",
    "\n",
    "# iterate through each subfile (topic) in the corpus directory\n",
    "for topic in os.listdir(corpus_directory):\n",
    "    topic_path = os.path.join(corpus_directory, topic)\n",
    "    \n",
    "    if os.path.isdir(topic_path):  # if it's a directory (subfile)\n",
    "        # Iterate through each CSV file in the subfile (topic)\n",
    "        for file in os.listdir(topic_path):\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(topic_path, file)\n",
    "                data = pd.read_csv(file_path)\n",
    "            \n",
    "                # Collecting all words across the dataset\n",
    "                for column in data.columns:\n",
    "                    if column.startswith('Answer.sentence'):\n",
    "                        words = data[column].astype(str).str.split().values\n",
    "                        for sentence in words:\n",
    "                            unique_words.update(sentence)\n",
    "\n",
    "# calculate the total vocabulary size\n",
    "vocabulary_size = len(unique_words)\n",
    "\n",
    "print(\"Vocabulary Size:\")\n",
    "print(f\"Total Vocabulary Size: {vocabulary_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820ff03",
   "metadata": {},
   "source": [
    "#### Vocabulary size per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7253381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size Per Topic:\n",
      "Topic: action, Vocabulary Size: 5932\n",
      "Topic: baseball, Vocabulary Size: 10761\n",
      "Topic: basketball, Vocabulary Size: 9406\n",
      "Topic: beatles, Vocabulary Size: 7554\n",
      "Topic: comedy, Vocabulary Size: 5647\n",
      "Topic: disney, Vocabulary Size: 17097\n",
      "Topic: fashion, Vocabulary Size: 6761\n",
      "Topic: fast_furious, Vocabulary Size: 4321\n",
      "Topic: harry_potter, Vocabulary Size: 5750\n",
      "Topic: horror, Vocabulary Size: 6310\n",
      "Topic: icehockey, Vocabulary Size: 4882\n",
      "Topic: lady_gaga, Vocabulary Size: 5773\n",
      "Topic: movies, Vocabulary Size: 44744\n",
      "Topic: music, Vocabulary Size: 49886\n",
      "Topic: music_and_movies, Vocabulary Size: 6314\n",
      "Topic: nfl_football, Vocabulary Size: 28460\n",
      "Topic: pop, Vocabulary Size: 7839\n",
      "Topic: rap_hiphop, Vocabulary Size: 8427\n",
      "Topic: rock, Vocabulary Size: 8317\n",
      "Topic: star_wars, Vocabulary Size: 14013\n",
      "Topic: superhero, Vocabulary Size: 5712\n",
      "Topic: thriller, Vocabulary Size: 9134\n",
      "Topic: transition_music_movies, Vocabulary Size: 1270\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "corpus_directory = 'C:/Users/nikia/Desktop/ΔΠΜΣ-ΓΛΩΣΣΙΚΗ ΤΕΧΝΟΛΟΓΙΑ/3ο εξάμηνο/Διαλογικά Συστήματα +Φωνητικοί Βοηθοί/εργασιες/2η/self_dialogue_corpus-main/self_dialogue_corpus-main/corpus/'\n",
    "\n",
    "# initialization of a dictionary to store unique words per topic\n",
    "topic_unique_words = {}\n",
    "\n",
    "# iterate through each subfile (topic) in the corpus directory\n",
    "for topic in os.listdir(corpus_directory):\n",
    "    topic_path = os.path.join(corpus_directory, topic)\n",
    "    \n",
    "    if os.path.isdir(topic_path):  # if it's a directory (subfile)\n",
    "        unique_words = set()  # initialize a set for each topic\n",
    "        \n",
    "        # iterate through each CSV file in the subfile (topic)\n",
    "        for file in os.listdir(topic_path):\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(topic_path, file)\n",
    "                data = pd.read_csv(file_path)\n",
    "                \n",
    "                # collecting all words across the dataset for each topic\n",
    "                for column in data.columns:\n",
    "                    if column.startswith('Answer.sentence'):\n",
    "                        words = data[column].astype(str).str.split().values\n",
    "                        for sentence in words:\n",
    "                            unique_words.update(sentence)\n",
    "        \n",
    "        # calculate the total vocabulary size for each topic\n",
    "        topic_unique_words[topic] = len(unique_words)\n",
    "\n",
    "print(\"Vocabulary Size Per Topic:\")\n",
    "for topic, vocab_size in topic_unique_words.items():\n",
    "    print(f\"Topic: {topic}, Vocabulary Size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a14da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
